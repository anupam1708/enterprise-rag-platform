from typing import TypedDict, Annotated, Sequence, Optional
import operator
import os
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolExecutor, ToolInvocation
from langgraph.checkpoint.postgres import PostgresSaver
from psycopg_pool import ConnectionPool
from dotenv import load_dotenv
import json
import logging

# Configure logging for production visibility
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

# ============================================================
# ðŸŽ¯ ARCHITECT'S NOTE: State Persistence Layer
# ============================================================
# This is what separates a demo from production.
# - Conversations survive container restarts
# - Can "rewind" to any previous state (time travel debugging)
# - Enables human-in-the-loop workflows
# - Supports multi-tenant isolation via thread_id
# ============================================================

# 1. Define the "State"
# This is the "Memory" passed between nodes. It's just a list of messages.
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]

# 2. Setup Tools & Model

# ============================================================
# ðŸš¨ HITL-ENABLED TOOLS: These require human approval
# ============================================================

@tool
def buy_stock(symbol: str, quantity: int, price: float):
    """Execute a stock purchase. Requires human approval before execution.
    
    Args:
        symbol: Stock ticker symbol (e.g., 'GOOGL', 'AAPL')
        quantity: Number of shares to buy
        price: Maximum price per share
    """
    # In production, this would integrate with a brokerage API
    total_cost = quantity * price
    return f"âœ… EXECUTED: Bought {quantity} shares of {symbol} at ${price:.2f}/share. Total: ${total_cost:.2f}"

@tool
def send_email(to: str, subject: str, body: str):
    """Send an email. Requires human approval before execution.
    
    Args:
        to: Recipient email address
        subject: Email subject line
        body: Email body content
    """
    # In production, this would integrate with an email service
    return f"âœ… EXECUTED: Email sent to {to} with subject '{subject}'"

@tool
def delete_database_records(table: str, condition: str):
    """Delete records from a database table. DANGEROUS - Requires human approval.
    
    Args:
        table: Database table name
        condition: WHERE clause condition
    """
    # In production, this would execute actual database operations
    return f"âœ… EXECUTED: Deleted records from {table} WHERE {condition}"

# Safe tools that don't require approval
search = TavilySearchResults(max_results=5, search_depth="advanced", time_range="month")

# All tools (both safe and requiring approval)
tools = [search, buy_stock, send_email, delete_database_records]
tool_executor = ToolExecutor(tools)

# High-risk tools that require approval (HITL)
HITL_TOOLS = ["buy_stock", "send_email", "delete_database_records"]

model = ChatOpenAI(model="gpt-4o-mini", temperature=0)
# We bind tools to the model so it knows they exist (modern API)
# tool_choice="auto" means model decides when to use tools based on input
model = model.bind_tools(tools, tool_choice="auto")

# 3. Define Nodes (The Workers)

def call_model(state):
    """The Brain: Decides what to do next."""
    messages = state['messages']
    
    # Add system message to guide the agent
    if not any(isinstance(m, type(messages[0])) and hasattr(m, 'type') and getattr(m, 'type', None) == 'system' for m in messages):
        from langchain_core.messages import SystemMessage
        system_msg = SystemMessage(content="""You are an AI assistant with tools. You MUST use tools directly - never ask for permission first.

IMPORTANT RULES:
1. When a user requests an action (buy stock, send email, delete records), call the tool IMMEDIATELY
2. Do NOT say "I need your approval" or "Do you want to proceed" - just call the tool
3. The system automatically handles approvals for sensitive operations
4. You are expected to be proactive and use tools without hesitation

Examples:
- User: "Buy 100 shares of GOOGL at $150" â†’ IMMEDIATELY call buy_stock(symbol="GOOGL", quantity=100, price=150.0)
- User: "Send email to john@example.com saying hello" â†’ IMMEDIATELY call send_email(...)
- User: "Delete test records from users table" â†’ IMMEDIATELY call delete_database_records(...)

Do not overthink. Just use the tools.""")
        messages = [system_msg] + messages
        logger.info(f"ðŸ“ Added system message to guide tool usage")
    
    logger.info(f"ðŸ”„ Calling model with {len(messages)} messages")
    response = model.invoke(messages)
    logger.info(f"ðŸ“¤ Model response type: {type(response)}, has tool_calls: {hasattr(response, 'tool_calls')}")
    if hasattr(response, 'tool_calls'):
        logger.info(f"ðŸ”§ Tool calls: {response.tool_calls}")
    
    return {"messages": [response]}

def call_tool(state):
    """The Hands: Executes the tool if the Brain asked for it.
    
    NOTE: If this is a HITL tool, execution has already been approved
    by a human (via the interrupt pattern). We log this for audit.
    """
    messages = state['messages']
    last_message = messages[-1]
    
    # Parse the tool call (modern format)
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        tool_call = last_message.tool_calls[0]
        action = ToolInvocation(
            tool=tool_call["name"],
            tool_input=tool_call["args"],
        )
    else:
        # Fallback to old format if needed
        function_call = last_message.additional_kwargs["function_call"]
        action = ToolInvocation(
            tool=function_call["name"],
            tool_input=json.loads(function_call["arguments"]),
        )
    
    # Check if this is a high-risk tool that was approved
    if action.tool in HITL_TOOLS:
        logger.warning(f"ðŸš¨ EXECUTING APPROVED HIGH-RISK TOOL: {action.tool}")
        logger.warning(f"   Input: {action.tool_input}")
    else:
        logger.info(f"âš™ï¸ Executing Tool: {action.tool} with input: {action.tool_input}")
    
    # Execute
    response = tool_executor.invoke(action)
    
    # Create a message to send back to the Brain
    from langchain_core.messages import ToolMessage
    tool_message = ToolMessage(content=str(response), tool_call_id=tool_call.get("id", "default") if hasattr(last_message, 'tool_calls') else "default")
    
    return {"messages": [tool_message]}

# 4. Define Logic (The Router)
def should_continue(state):
    last_message = state['messages'][-1]
    # If the LLM returned a tool call, go to "tools" (modern API)
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "continue"
    # Fallback to old function_call format
    if "function_call" in last_message.additional_kwargs:
        return "continue"
    # Otherwise, stop
    return "end"

# 5. Build the Graph (The Architecture)
workflow = StateGraph(AgentState)

workflow.add_node("agent", call_model)
workflow.add_node("action", call_tool)

workflow.set_entry_point("agent") # Start here

# Conditional Edge: agent -> action OR agent -> end
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END
    }
)

# Normal Edge: action -> agent (Loop back to Brain to interpret results)
workflow.add_edge("action", "agent")

# ============================================================
# ðŸš€ PRODUCTION UPGRADE: PostgreSQL Checkpointer + HITL
# ============================================================
# Global connection pool and checkpointer instance
_checkpointer: Optional[PostgresSaver] = None
_connection_pool: Optional[ConnectionPool] = None

def get_checkpointer() -> PostgresSaver:
    """
    Factory function for the checkpointer with connection pooling.
    This creates a singleton instance that's reused across requests.
    """
    global _checkpointer, _connection_pool
    
    if _checkpointer is None:
        db_url = os.getenv(
            "DATABASE_URL",
            "postgresql://postgres:password@postgres:5432/compliance_db"
        )
        
        logger.info(f"ðŸ”„ Initializing PostgreSQL Checkpointer: {db_url.split('@')[1] if '@' in db_url else 'localhost'}")
        
        # Create connection pool
        _connection_pool = ConnectionPool(
            conninfo=db_url,
            max_size=20,
            kwargs={"autocommit": True, "prepare_threshold": 0},
        )
        
        # Create checkpointer with the pool
        _checkpointer = PostgresSaver(_connection_pool)
        
        # Setup tables if they don't exist  
        _checkpointer.setup()
        logger.info("âœ… Checkpointer initialized and tables verified")
    
    return _checkpointer

# Compile graph WITH interrupt support for HITL workflows
# When the agent wants to call a HITL tool, execution pauses BEFORE the "action" node
def compile_graph_with_hitl():
    """
    Compile the graph with Human-in-the-Loop interrupt configuration.
    
    The interrupt_before=["action"] means:
    - When the agent decides to call a tool, execution pauses
    - The state is saved to PostgreSQL
    - A human must approve/reject before the tool executes
    """
    checkpointer = get_checkpointer()
    
    # ðŸŽ¯ ARCHITECT'S NOTE: interrupt_before
    # This is the magic that enables HITL workflows.
    # When any tool is about to execute, we pause and wait for human input.
    return workflow.compile(
        checkpointer=checkpointer,
        interrupt_before=["action"]  # Pause before executing ANY tool
    )

# Compile graph WITHOUT checkpointer initially (will be added at runtime)
app_graph_stateless = workflow.compile()

# 6. Enhanced Helper Functions with State Persistence + HITL

async def run_graph_agent(
    query: str, 
    thread_id: str = "default",
    checkpoint_id: Optional[str] = None,
    enable_hitl: bool = False
):
    """
    Execute the agent with persistent state and optional HITL.
    
    Args:
        query: The user's question
        thread_id: Unique conversation identifier (maps to user session)
        checkpoint_id: Optional - resume from a specific checkpoint (time travel)
        enable_hitl: If True, pauses before executing high-risk tools
    
    Returns:
        The agent's final response OR a pending approval status
    """
    # Use HITL-enabled or regular graph based on flag
    if enable_hitl:
        app_graph = compile_graph_with_hitl()
    else:
        checkpointer = get_checkpointer()
        app_graph = workflow.compile(checkpointer=checkpointer)
    
    # Configuration for this execution thread
    config = {
        "configurable": {
            "thread_id": thread_id,
        }
    }
    
    # If checkpoint_id provided, add it for time-travel
    if checkpoint_id:
        config["configurable"]["checkpoint_id"] = checkpoint_id
        logger.info(f"ðŸ•°ï¸ Time travel: resuming from checkpoint {checkpoint_id}")
    
    # Create input with the new message
    inputs = {"messages": [HumanMessage(content=query)]}
    
    logger.info(f"ðŸ§  Agent processing query for thread: {thread_id} (HITL: {enable_hitl})")
    
    # Use invoke (sync) but wrap in async for FastAPI
    # The graph handles state loading/saving automatically
    result = app_graph.invoke(inputs, config=config)
    
    # If HITL is enabled, check if we hit an interrupt
    if enable_hitl:
        state = app_graph.get_state(config)
        logger.info(f"ðŸ›‘ After invoke: state.next={state.next}")
        if state.next:  # Paused at interrupt
            # Extract pending tool call details
            for msg in reversed(result['messages']):
                if hasattr(msg, 'tool_calls') and msg.tool_calls:
                    tool_call = msg.tool_calls[0]
                    logger.info(f"ðŸŽ¯ HITL PAUSED: Pending tool={tool_call['name']}")
                    return {
                        "answer": f"â¸ï¸ WAITING FOR APPROVAL: I want to execute {tool_call['name']} with parameters: {tool_call['args']}",
                        "pending_approval": True,
                        "tool_name": tool_call['name'],
                        "tool_input": tool_call['args']
                    }
    
    return result['messages'][-1].content

async def check_pending_approval(thread_id: str):
    """
    Check if a conversation is waiting for human approval.
    
    Returns:
        Dict with status and pending action details, or None if not waiting
    """
    app_graph = compile_graph_with_hitl()
    config = {"configurable": {"thread_id": thread_id}}
    
    # Get the current state
    state = app_graph.get_state(config)
    
    logger.info(f"ðŸ” Checking pending for thread {thread_id}: state.next={state.next}")
    
    # Check if we're at an interrupt point
    if state.next:  # next contains the nodes that are ready to execute
        # We're paused before a node
        messages = state.values.get("messages", [])
        logger.info(f"ðŸ” Found {len(messages)} messages in state")
        if messages:
            last_message = messages[-1]
            logger.info(f"ðŸ” Last message type: {type(last_message)}, has tool_calls: {hasattr(last_message, 'tool_calls')}")
            
            # Check for tool_calls (modern format)
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                tool_call = last_message.tool_calls[0]
                tool_name = tool_call["name"]
                tool_input = tool_call["args"]
                
                logger.info(f"âœ… Found pending tool: {tool_name}")
                
                return {
                    "status": "pending_approval",
                    "tool_name": tool_name,
                    "tool_input": tool_input,
                    "thread_id": thread_id,
                    "is_high_risk": tool_name in HITL_TOOLS,
                    "next_node": state.next[0] if state.next else None
                }
            
            # Fallback to old function_call format
            elif hasattr(last_message, 'additional_kwargs') and "function_call" in last_message.additional_kwargs:
                function_call = last_message.additional_kwargs["function_call"]
                tool_name = function_call["name"]
                tool_input = json.loads(function_call["arguments"])
                
                logger.info(f"âœ… Found pending tool (old format): {tool_name}")
                
                return {
                    "status": "pending_approval",
                    "tool_name": tool_name,
                    "tool_input": tool_input,
                    "thread_id": thread_id,
                    "is_high_risk": tool_name in HITL_TOOLS,
                    "next_node": state.next[0] if state.next else None
                }
    
    logger.info(f"â„¹ï¸  No pending approval for thread {thread_id}")
    return {"status": "not_waiting"}

async def approve_and_continue(thread_id: str, approved: bool = True):
    """
    Approve or reject a pending tool execution and continue.
    
    Args:
        thread_id: The conversation thread
        approved: True to approve, False to reject
    
    Returns:
        The result after approval/rejection
    """
    app_graph = compile_graph_with_hitl()
    config = {"configurable": {"thread_id": thread_id}}
    
    if not approved:
        # Rejection: Add a message explaining the rejection and end
        logger.warning(f"âŒ Tool execution REJECTED by human for thread: {thread_id}")
        
        from langchain_core.messages import AIMessage
        rejection_msg = AIMessage(content="âŒ Action cancelled by user. The requested operation was not executed.")
        
        app_graph.update_state(config, {"messages": [rejection_msg]})
        
        return {
            "status": "rejected",
            "message": "âŒ Action cancelled by user. The requested operation was not executed."
        }
    
    # Approval: Continue execution
    logger.info(f"âœ… Tool execution APPROVED by human for thread: {thread_id}")
    
    # Resume from the interrupt by invoking with None
    # This continues from where we left off
    result = app_graph.invoke(None, config=config)
    
    return {
        "status": "approved",
        "result": result['messages'][-1].content
    }


async def get_conversation_history(thread_id: str, limit: int = 10):
    """
    Retrieve conversation history for a thread.
    
    Args:
        thread_id: The conversation thread
        limit: Maximum number of checkpoints to retrieve
    
    Returns:
        List of states with metadata
    """
    checkpointer = get_checkpointer()
    app_graph = workflow.compile(checkpointer=checkpointer)
    
    config = {"configurable": {"thread_id": thread_id}}
    
    history = []
    state_iterator = app_graph.get_state_history(config)
    
    count = 0
    for state in state_iterator:
        if count >= limit:
            break
        
        history.append({
            "checkpoint_id": state.config["configurable"].get("checkpoint_id"),
            "messages": [
                {"type": msg.type, "content": msg.content}
                for msg in state.values.get("messages", [])
            ],
            "metadata": state.metadata,
            "parent_config": state.parent_config
        })
        count += 1
    
    return history

async def rewind_conversation(thread_id: str, steps_back: int = 1):
    """
    Time-travel debugging: Rewind the conversation N steps back.
    Useful for correcting errors or exploring alternate paths.
    
    Args:
        thread_id: The conversation thread
        steps_back: Number of steps to rewind
    
    Returns:
        The checkpoint_id to resume from
    """
    history = await get_conversation_history(thread_id, limit=steps_back + 1)
    
    if len(history) <= steps_back:
        raise ValueError(f"Cannot rewind {steps_back} steps - only {len(history)} checkpoints available")
    
    target_checkpoint = history[steps_back]
    logger.info(f"âª Rewound {steps_back} steps to checkpoint: {target_checkpoint['checkpoint_id']}")
    
    return target_checkpoint['checkpoint_id']

# ============================================================
# ðŸŽ“ ARCHITECT'S INTERVIEW TALKING POINTS:
# ============================================================
# 1. "I implemented PostgreSQL-backed state persistence using LangGraph's
#    AsyncPostgresSaver, which serializes the entire graph state after
#    each node execution."
#
# 2. "This enables production features like conversation resumption after
#    container restarts, time-travel debugging by rewinding to any
#    checkpoint, and human-in-the-loop workflows where agents pause
#    for approval."
#
# 3. "The architecture uses thread_id for multi-tenant isolation - each
#    user's conversation is stored independently."
#
# 4. "Connection pooling is handled at the checkpointer level using
#    asyncpg for non-blocking I/O under load."
# ============================================================
